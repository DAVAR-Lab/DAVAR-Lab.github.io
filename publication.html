<!DOCTYPE html>
<html lang="en">
<style type="text/css">
.unreleased{
	color: #C0C0C0
}
a{
	padding-right: 2px
}

</style>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">

    <title> DAVAR LAB</title>
    <meta name="description" content="">
    <link rel="alternate" type="application/rss+xml" href="feed.xml">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/css/main.css" rel='stylesheet' type='text/css'>

    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Nunito:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic' rel='stylesheet'
        type='text/css'>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/small_logo.png">
</head>

<body data-spy="scroll" data-offset="80" data-target=".scrollspy" id="top">
    <div class="navigation"></div>
    <div class="news top-container">
        <div class="container-fluid">
            <div class="post">
                
               
                <h2>Conference Papers</h2>
				
				 <!--ECCV 2022 PGM-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/eccv2022_pgm.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/eccv2022_pgm.html">Distilling Object Detectors With Global Knowledge</a>                
                            <br>
                            <small>Sanli Tang, Zhongyu Zhang, Zhanzhan Cheng, Jing Lu, Yunlu Xu, Yi Niu, Fan He</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ECCV 2022 </font>
                            </strong>
			
                            <br>
                            <a href="/files/eccv2022_distill/2717.pdf">[Paper]</a>
							<a href="https://github.com/hikvision-research/DAVAR-Lab-ML/tree/main/ECCV22_Distilling_Object_Detectors_with_Global_Knowledge">[Code] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				
				 <!--ECCV 2022 DLD-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/eccv2022_dld.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/eccv2022_dld.html">Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting</a>                
                            <br>
                            <small>Ying Chen<sup>*</sup>, Liang Qiao<sup>*</sup>, Zhanzhan Cheng, Shiliang Pu<sup>†</sup>, Yi Niu, Xi Li<sup>†</sup></small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ECCV 2022 </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06694.pdf" target="_blank" 
                               style="color: #990000">[Paper] </a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_spotting/dld" target="_blank" 
                               style="color: #990000">[Code] </a>
							<a href="/files/eccv2022/DLD-poster.pdf" target="_blank" 
                               style="color: #990000">[Poster] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				
				
				 <!--ACM MM 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2022_davarocr.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/acmmm2022_davarocr.html">DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding</a>                
                            <br>
                            <small>Liang Qiao, Hui Jiang, Ying Chen, Can Li, Pengfei Li, Zaisheng Li, Baorui Zou, Dashan Guo, Yingda Xu, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Yi Niu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2022 (Open-Source Software Competition) </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06695.pdf" target="_blank" 
                               style="color: #990000">[Paper] </a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR" target="_blank" 
                               style="color: #990000">[Code] </a>
							<a href="" target="/files/mm2022_davarocr/DavarOCR-poster.pdf" 
                               style="color: #990000">[Poster] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				 <!--ACM MM 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2022_ctunet.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/acmmm2022_ctunet.html">End-to-End Compound Table Understanding with Multi-Modal Modeling</a>                
                            <br>
                            <small>Zaisheng Li<sup>*</sup>, Yi Li<sup>*</sup>, Liang Qiao<sup>*</sup>, Pengfei Li, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Xi Li<sup>†</sup></small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2022 (Oral) </font>
                            </strong>
			
                            <br>
                            <a class="unreleased">[Paper] (To be published)</a>
							<a class="unreleased" >[Code] (To be published) </a>
					
                        </small>
                    </div>
                </div>
                <hr /> 
        
				 <!--AAAI 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2022_open.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/aaai2022_pmal.html">PMAL: Open Set Recognition via Robust Prototype Mining</a>                
                            <br>
                            <small>Jing Lu<sup>*</sup>, Yunlu Xu<sup>*</sup>, Hao Li, Zhanzhan Cheng<sup>†</sup>, Yi Niu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2022  </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2203.08569.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a class="unreleased" >[Code] (To be published)</a>
							<a href="/files/aaai2022_pmal/pre-PMAL.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="/files/aaai2022_pmal/poster_PMAL.pdf" target="_blank" 
                               style="color: #990000">[Poster]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
        
				<!--BMVC 2021_FSL -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/bmvc2021.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/bmvc2021_FSL.html">A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning</a>                
                            <br>
                            <small>Linglan Zhao, Dashan Guo, Yunlu Xu<sup>†</sup>, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Xiangzhong Fang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">BMVC 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2110.11128.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
				
			
				
				
			<!--ICDAR 2021_LGPMA -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_lgpma.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_lgpma.html">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment</a>                
                            <br>
                            <small>Liang Qiao<sup>*</sup>, Zaisheng Li<sup>*</sup>, Zhanzhan Cheng<sup>†</sup>, Peng Zhang, Shiliang Pu, Yi Niu, Wenqi Ren, Wenming Tan, and Fei Wu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral), Best Industry Paper</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06224.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/table_recognition/lgpma" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/icdar2021_lgpma/LGPMA-slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV19Q4y1Y73d/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
		
		<!--ICDAR 2021_VSR -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_vsr.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_vsr.html">VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations</a>                
                            <br>
                            <small>Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu and Fei Wu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06220.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_layout/VSR" target="_blank" style="color: #990000" >[Code] </a>
							<a href="/files/icdar2021_vsr/VSR-Slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV1ff4y1n7zm/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 

		
		
		<!--ICDAR 2021_RF Learning -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_rf.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_rf.html">Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition</a>                
                            <br>
                            <small>Hui Jiang*, Yunlu Xu*, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu, Wenqi Ren, Fei Wu, Wenming Tan</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06229.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_recognition/rflearning" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/icdar2021_rfl/RFL-slide.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV1dq4y1M7kL/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
				
				
				

				<!-- AAAI 2021_MANGO -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2021_mango.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2021_mango.html">MANGO: A Mask Attention Guided One-Stage Scene Text Spotter

</a>                       
                            <br>
                            <small>Liang Qiao<sup>*</sup>, Ying Chen<sup>*</sup>, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu<sup>†</sup>, Fei Wu</small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2012.04350.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_spotting/mango" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/aaai2021_mango/MANGO-poster.pdf" target="_blank" 
                               style="color: #990000">[Poster]</a>
							<a href="/files/aaai2021_mango/MANGO-slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
                       
                        </small>
                    </div>
                </div>
                <hr /> 
		    
		<!-- AAAI 2021_SPIN -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2021_spin.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2021_spin.html">SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition
</a>                       
                            <br>
                            <small>Chengwei Zhang*, Yunlu Xu*, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2005.13117.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_recognition/spin" target="_blank" style="color: #990000">[Code] </a>
                       
                        </small>
                    </div>
                </div>
                <hr /> 
		<!-- ICPR 2020 FASDA-->
		<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icpr2020_fasda.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/icpr2020_fasda.html">Text Recognition in Real Scenarios with a Few Labeled Samples</a>                       
                            <br>
                            <small>Jinghuang Lin, Zhanzhan Cheng, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ICPR 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2006.12209.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr /> 
		<!--ICPR 2020 PEE-->
		 <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icpr2020_pee.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/icpr2020_pee.html">Recognizing Multiple Text Sequences from an Image by Pure End-To-End Learning</a>                       
                            <br>
                            <small>Zhenlong Xu, Shuigeng Zhou<sup>†</sup>, Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu</small>
                        </h4>

                        <small>Accepted by 
                            <strong>
                                <font color="red">ICPR 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1907.12791.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr /> 
		    
		<!-- AAM MM 2020_trie -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2020_trie.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2020_trie.html">TRIE: End-to-End Text Reading and Information Extraction for
Document Understanding</a>                       
                            <br>
                            <small>Peng Zhang<sup>*</sup>, Yunlu Xu<sup>*</sup>, Zhanzhan Cheng, Shiliang Pu<sup>†</sup>, Jing Lu, Liang Qiao, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2005.13118.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_ie/trie" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/mm20_trie/TRIE_slides.pdf" target="_blank"
                                style="color: #990000">[Slides]</a>
                        </small>
                    </div>
                </div>
                <hr />  
		    
                
                <!-- AAAI 2020_tp -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2020_tp.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2020_tp.html">Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting</a>                       
                            <br>
                            <small>Liang Qiao, Sanli Tang, Zhanzhan Cheng<sup>†</sup>, Yunlu Xu, Yi Niu, Shiliang Pu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2020 (Oral)</font>
                            </strong>
                            <br>
 		    <a href="https://arxiv.org/pdf/2002.06820" target="_blank" 
                               style="color: #990000">[Paper]</a>
                            <a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_detection/text_perceptron_det" target="_blank"
                                style="color: #990000">[Code]</a>
							<a href="/files/aaai2020_tp/AAAI20-TP-Supplementary.pdf" target="_blank"
                                style="color: #990000">[Supplementary]</a>
								<a href="/files/aaai2020_tp/AAAI20-TP-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
								<a href="https://www.bilibili.com/video/BV1Wz4y1k7Dc" target="_blank"
                                style="color: #990000">[Presentation Video]</a>                       
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
		<!--  PRCV 2019_REAPS -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/prcv2019_reaps.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/prcv2019_reaps.html">REAPS: Towards Better Recognition of Fine-grained Images by Region Attending and Part Sequencing</a>                       
                            <br>
                            <small>Peng Zhang, Xinyu Zhu, Zhanzhan Cheng<sup>†</sup>, Shuigeng Zhou, Yi Niu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">PRCV 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1908.01962.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a> 
							<a href="/files/pcrv19_reaps/prcv19-REAPS-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
                        </small>
                    </div>
                </div>
                <hr />  
		    
		
		<!--  ACMMM 2019_YORO -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2019_yoro.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2019_yoro.html">You Only Recognize Once: Towards Fast Video Text Spotting</a>                       
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Jing Lu<sup>*</sup>, Yi Niu, Shiliang Pu, Fei Wu<sup>†</sup>, Shuigeng Zhou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1903.03299.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>  
							   <a href="dataset/lsvtd.html" target="_blank" 
                               style="color: #990000">[Dataset]</a>  
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/videotext/yoro" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/mm19_yoro/MM19-YORO-poster.pdf" style="color: #990000" target="_blank">[Poster]</a>
							<a href="/files/mm19_yoro/MM19-YORO-spotlights.pdf" style="color: #990000" target="_blank">[Spotlights]</a>
							<a href="https://www.bilibili.com/video/bv1gK4y1L7No" target="_blank" style="color:#990000">[Demo]</a>
                        </small>
                    </div>
                </div>
                <hr />
                

		<!--  ACMMM 2019_SSG -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2019_ssg.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2019_ssg.html">Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal Action Localization</a>                       
                            <br>
                            <small>Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Yi Niu, Shiliang Pu, Fei Wu, Futai Zou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1908.02422.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							   <a href="/files/mm19_assg/MM19-ASSG-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>  
								<a href="/files/mm19_assg/MM19-ASSG-spotlights.pdf" target="_blank"
                                style="color: #990000">[Spotlights]</a>								
                        </small>
                    </div>
                </div>
                <hr />
		    
		    
		                <!-- AAAI 2019_star -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2019_star.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2019_star.html">Segregated Temporal Assembly Recurrent Networks for Weakly Supervised Multiple Action Detection</a>                       
                            <br>
                            <small>Yunlu Xu<sup>*</sup>, Chengwei Zhang<sup>*</sup>, Zhanzhan Cheng<sup>†</sup>, Jianwen Xie, Yi Niu, Shiliang Pu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2019 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1811.07460.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							 <a href="/files/aaai2019_star/AAAI19-Star-Slides.pdf" target="_blank"
                                style="color: #990000">[Slides]</a>  
                        </small>
                    </div>
                </div>
                <hr />  
		    
		 
				                <!-- CVPR 2018_EPL -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2018_epl.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/cvpr2018_epl.html">Edit Probability for Scene Text Recognition</a>                       
                            <br>
                            <small>Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2018</font>
                            </strong>
                            <br>
                            <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Edit_Probability_for_CVPR_2018_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
						                <!-- CVPR 2018_AON -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2018_aon.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/cvpr2018_aon.html">AON: Towards Arbitrarily-Oriented Text Recognition</a>                       
                            <br>
                            <small>Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2018</font>
                            </strong>
                            <br>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="/files/cvpr18_aon/cvpr18_poster_AON.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
								                <!-- ICCV 2017_FAN -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/iccv2017_fan.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/iccv2017_fan.html">Focusing Attention: Towards Accurate Text Recognition in Natural Images</a>                       
                            <br>
                            <small>Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ICCV 2017</font>
                            </strong>
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="/files/iccv17_fan/iccv17_poster_FAN.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
							
                        </small>
                    </div>
                </div>
                <hr />  
				<h2>Cooperation Papers</h2> 
				
				<!--COLING2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/COLING2022.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="#">Read Extensively, Focus Smartly: A Cross-document Semantic
Enhancement Method for Visual Documents NER</a>                
                            <br>
                            <small>Jun Zhao, Xin Zhao, Wenyu Zhan, Tao Gui, Qi Zhang, Liang Qiao, Zhanzhan Cheng, Shiliang Pu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">COLING 2022 </font>
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="https://aclanthology.org/2022.coling-1.177.pdf" target="_blank" 
                               style="color: #990000">[Paper] </a>
                        </small>
                    </div>
                </div>
				<hr />
				
				<!--KDD2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/kdd2022.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="#">Active Model Adaptation Under Unknown Shift</a>                
                            <br>
                            <small>Jiejing Shao, Yunlu Xu, Zhanzhan Cheng, Yufeng Li</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">KDD 2022 </font>
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="https://dl.acm.org/doi/10.1145/3534678.3539262" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
				<hr />
				
				 <!--acl2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/acl2022_miner.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="#">MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective</a>                
                            <br>
                            <small>Xiao Wang, Shihan Dou, Limao Xiong, Yicheng Zou, Qi Zhang, Tao Gui, Liang Qiao, Zhanzhan Cheng, Xuanjing Huang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACL 2022 </font>
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="https://arxiv.org/pdf/2204.04391.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
				<hr />
				 <!--acl2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/acl2022_flooding.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="#">Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning</a>                
                            <br>
                            <small>Qin Liu, Rui Zheng, Rong Bao, Jingyi Liu, ZhiHua Liu, Zhanzhan Cheng, Liang Qiao, Tao Gui, Qi Zhang, Xuanjing Huang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACL 2022 </font>
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="http://qizhang.info/paper/acl2022.Flooding.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
				<hr />
				
				 <!--NeurIPS 2021 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/neurips2021.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/ijcai2021.html">STEP : Out-of-Distribution Detection in the Presence of Limited In-distribution Labeled Data</a>                
                            <br>
                            <small>Zhi Zhou, Lanzhe Guo, Zhanzhan Cheng, Yufeng Li<sup>†</sup>, Shiliang Pu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">NeurIPS 2021 </font>
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="https://proceedings.neurips.cc/paper/2021/file/f4334c131c781e2a6f0a5e34814c8147-Paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
				<!--IJCAI 2021 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="img/publications/ijcai2021.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/ijcai2021.html">Towards Robust Model Reuse in the Presence of Latent Domains</a>                
                            <br>
                            <small>Jiejing Shao, Zhanzhan Cheng, Yufeng Li<sup>†</sup>, Shiliang Pu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">IJCAI 2021 </font> 
                            </strong>
							(cooperation paper)
                            <br>
                            <a href="https://cs.nju.edu.cn/liyf/paper/ijcai21-MRL.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
                <hr /> 	
				
				
				 <h2>Journal Papers</h2> 
                <hr />                  
				
				<!-- tip2021_free -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/tip2021_free.png" width="1000">
                        </div>


                        <h4 class="media-heading">
                            <a href="/publication/tip2021_free.html">FREE: A Fast and Robust End-to-End Video Text Spotter</a>  
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Jing Lu<sup>*</sup>, Baorui Zou, Liang Qiao, Yunlu Xu, Shiliang Pu<sup>†</sup>, Yi Niu, Fei Wu, Shuigeng Zhou</small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">TIP 2021</font>
                            </strong>
							<br>
							<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9266586" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                        <br>
                    </div>
                </div>
                <hr />
				
				
		
                <h2>Technique Reports</h2> 
                <hr />   
				<!-- iccv2021_sslad_b -->
				<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/iccv2021_workshop_sslad_1.png" width="1000">
                        </div>


                        <h4 class="media-heading">
                            <a href="/competition/SSLAD-Track-1">2nd Place Solution SSLAD Track 1 - O2O Semi-Supervised Framwork</a>  
                            <br>
                            <small>Beitong Zhou<sup>*</sup>, Donghao Gu<sup>*</sup>, Binbin Zhang<sup>*</sup>, Sanli Tang, Yunlu Xu<sup>†</sup>, Zhanzhan Cheng, Yi Niu</small>
                        </h4>

                        <small>Technique report of  
                            <strong>
                                <font color="red">ICCV 2021 Workshop SSLAD Track-1</font>
                            </strong>
							competition.
							<br>
							<a href="/files/iccv2021sslad/ICCV2021-SSLAD Track1-2nd.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                        <br>
                    </div>
                </div>
				<!-- iccv2021_sslad_b -->
				<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/iccv2021_workshop_sslad_b.png" width="1000">
                        </div>


                        <h4 class="media-heading">
                            <a href="/competition/SSLAD-Track-3">Technical Report for ICCV 2021 Challenge SSLAD-Track3B: Transformers Are Better Continual Learners</a>  
                            <br>
                            <small>Duo Li<sup>*</sup>, Guimei Cao<sup>*</sup>, Yunlu Xu, Zhanzhan Cheng, Yi Niu</small>
                        </h4>

                        <small>Technique report of  
                            <strong>
                                <font color="red">ICCV 2021 Workshop SSLAD Track-3B</font>
                            </strong>
							competition.
							<br>
							<a href="/files/iccv2021sslad/sslad-report-3B.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                        <br>
                    </div>
                </div>
				<!-- icdar2021_svts -->
				<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/competitions/icdar2021svts.png" width="1000">
                        </div>


                        <h4 class="media-heading">
                            <a href="/competition/icdar2021svts.html">ICDAR 2021 Competition on Scene Video Text Spotting</a>  
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Jing Lu<sup>*</sup>, Baorui Zou<sup>*</sup>, Shuigeng Zhou, Fei Wu</small>
                        </h4>

                        <small>Technique report of  
                            <strong>
                                <font color="red">ICDAR 2021 SVTS </font>
                            </strong>
							competition
							<br>
							<a href="https://arxiv.org/pdf/2107.11919.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="/files/icdar2021_svts/Comp-ST_01.pdf" target="_blank" 
                               style="color: #990000">[Poster]</a>
							<a href="https://www.bilibili.com/video/BV1ZU4y1n7ZF/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                        <br>
                    </div>
                </div>
				<!-- icdar2021_svts -->
				<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_rrc.png" width="1000">
                        </div>


                        <h4 class="media-heading">
                            <a href="/competition/icdar2021rrc.html">ICDAR 2021 Competition on Robust Reading Challenge - ICText</a>  
                            <br>
                            <small>Qiyao Wang<sup>*</sup>, Pengfei Li<sup>*</sup>, Li Zhu, Yi Niu</small>
                        </h4>

                        <small>Technique report of  
                            <strong>
                                <font color="red">ICDAR 2021 RRC-ICText </font>
                            </strong>
							competition
							<br>
							<a href="https://arxiv.org/pdf/2104.03544.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							
                        </small>
                        <br>
                    </div>
                </div>

               
				
				<h2>Manuscripts (Preprints)</h2> 
                <hr />
				<!-- arxiv 2022  TRIE++ -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/arxiv2022_e2-aen.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/arxiv2022_e2-aen.html">E2-AEN: End-to-End Incremental Learning with Adaptively Expandable Network</a>                       
                            <br>
                            <small>Guimei Cao<sup>*</sup>, Zhanzhan Cheng<sup>*</sup>, Yunlu Xu<sup>*</sup>, Duo Li, Shiliang Pu, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Available on
                            <strong>
                                <font color="red">arXiv</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06754.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />
				<!-- arxiv 2022  TRIE++ -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/arxiv2022_trie++.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/arxiv2022_trie++.html">TRIE++: Towards End-to-End Information Extraction from Visually Rich Documents</a>                       
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Peng Zhang<sup>*</sup>, Can Li<sup>*</sup>, Qiao Liang, Yunlu Xu, Pengfei Li, Shiliang Pu, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Available on
                            <strong>
                                <font color="red">arXiv</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06744.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
				
				
				<!-- arxiv 2020_da -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/arxiv2020_da.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/arxiv2020_da.html">Learning a Domain Classifier Bank for Unsupervised Adaptive Object Detection</a>                       
                            <br>
                            <small>Sanli Tang, Zhanzhan Cheng, Shiliang Pu, Dashan Guo, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Available on
                            <strong>
                                <font color="red">arXiv</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2007.02595" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  

				
				<!-- arxiv 2020_objectqa -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/arxiv2020_objectqa.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/arxiv2020_objectqa.html">Object-QA: Towards High Reliable Object Quality Assessment</a>                       
                            <br>
                            <small>Jing Lu<sup>*</sup>, Baorui Zou<sup>*</sup>, Zhanzhan Cheng<sup>*</sup>, Shiliang Pu, Shuigeng Zhou, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Available on
                            <strong>
                                <font color="red">arXiv</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2005.13116.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
                
				<!-- arxiv 2020_rg -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/arxiv2020_rg.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/arxiv2020_rg.html">Refined Gate: A Simple and Effective Gating Mechanism for Recurrent Units</a>                       
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Yunlu Xu<sup>*</sup>, Mingjian Cheng, Yu Qiao, Shiliang Pu, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Available on
                            <strong>
                                <font color="red">arXiv</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2002.11338.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
				
				

            </div>
        </div>
    </div>


    <div class="footer"></div>
    <script src="/js/jquery.min.js"></script>
    <script src="/js/all.min.js"></script>

    <script>
        $(document).ready(function () {
            $(".footer").load("/common/footer.html");
            $(".navigation").load("/common/navigation.html");
        });
    </script>
</body>

</html>

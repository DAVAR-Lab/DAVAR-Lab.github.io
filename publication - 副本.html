<!DOCTYPE html>
<html lang="en">
<style type="text/css">
.unreleased{
	color: #C0C0C0
}
a{
	padding-right: 2px
}
.col-xs-3 img{
	min-height: 90px
}
</style>
<style>
        .tab {
            margin: 0 auto;
            width: 800px;
            padding-top: 20px;
        }
        
        .tab_list {
            height: 50px;
            border-bottom: 1px solid red;
        }
        
        .tab_list ul {
            padding-left: 0px;
            height: 50px;
        }
        
        .tab li {
            float: left;
            display: inline-block;
            box-sizing: border-box;
            padding-left: 20px;
            padding-right: 20px;
            text-align: center;
            height: 50px;
            line-height: 50px;
            margin: 0;
        }
        
        .item {
            display: none;
        }
        
        .current {
            background-color: brown;
            color: white;
        }
    </style>

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">

    <title> DAVAR LAB</title>
    <meta name="description" content="">
    <link rel="alternate" type="application/rss+xml" href="feed.xml">

</head>

<body data-spy="scroll" data-offset="80" data-target=".scrollspy" id="top">
    <div class="navigation"></div>
    <div class="news top-container">
       <div class="tab">
        <div class="tab_list">
            <ul>
                <li class="current">商品介绍</li>
                <li>规格与包装</li>
                <li>商品评价(100万+)</li>
                <li>售后保障</li>
 
            </ul>
        </div>
        <div class="tab_con">
            <div class="item current">
                <!--ICME 2023 FSCIL-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icme2023_fscil.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icme2023_fscil.html">Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation</a>                
                            <br>
                            <small>Linglan Zhao, Jing Lu, Zhanzhan Cheng, Duo Liu, Xiangzhong Fang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICME 2023 </font>
                            </strong>
			
                            <br>
                            <a href="/files/icme2023_fscil/ICME_2023_Camera_Ready.pdf">[Paper]</a>

                        </small>
                    </div>
                </div>
                <hr />
				
				 <!--CVPR 2023 FSCIL-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2023_hyper.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/cvpr2023_hyper.html">Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation</a>                
                            <br>
                            <small>Beitong Zhou<sup>*</sup>, Jing Lu<sup>*</sup>, Kerui Liu, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Yi Niu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2023 </font>
                            </strong>
			
                            <br>
                            <a href="/files/cvpr2023_hypermatch/HyperMatch-Camera-Ready.pdf">[Paper]</a>

                        </small>
                    </div>
                </div>
                <hr />
				 <!--CVPR 2023 FSCIL-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2023_fscil.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/cvpr2023_fscil.html">Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation</a>                
                            <br>
                            <small>Linglan Zhao<sup>*</sup>,Jing Lu<sup>*</sup>, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Dashan Guo, Yi Niu, Xiangzhong Fang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2023 </font>
                            </strong>
			
                            <br>
                            <a href="/files/cvpr2023_fscil/FSCIL-Camera-Ready.pdf">[Paper]</a>

                        </small>
                    </div>
                </div>
                <hr />
				
				
				 <!--ECCV 2022 PGM-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/eccv2022_pgm.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/eccv2022_pgm.html">Distilling Object Detectors With Global Knowledge</a>                
                            <br>
                            <small>Sanli Tang, Zhongyu Zhang, Zhanzhan Cheng, Jing Lu, Yunlu Xu, Yi Niu, Fan He</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ECCV 2022 </font>
                            </strong>
			
                            <br>
                            <a href="/files/eccv2022_distill/2717.pdf">[Paper]</a>
							<a href="https://github.com/hikvision-research/DAVAR-Lab-ML/tree/main/ECCV22_Distilling_Object_Detectors_with_Global_Knowledge">[Code] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				
				 <!--ECCV 2022 DLD-->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/eccv2022_dld.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/eccv2022_dld.html">Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting</a>                
                            <br>
                            <small>Ying Chen<sup>*</sup>, Liang Qiao<sup>*</sup>, Zhanzhan Cheng, Shiliang Pu<sup>†</sup>, Yi Niu, Xi Li<sup>†</sup></small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ECCV 2022 </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06694.pdf" target="_blank" 
                               style="color: #990000">[Paper] </a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_spotting/dld" target="_blank" 
                               style="color: #990000">[Code] </a>
							<a href="/files/eccv2022/DLD-poster.pdf" target="_blank" 
                               style="color: #990000">[Poster] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				
				
				 <!--ACM MM 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2022_davarocr.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/acmmm2022_davarocr.html">DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding</a>                
                            <br>
                            <small>Liang Qiao, Hui Jiang, Ying Chen, Can Li, Pengfei Li, Zaisheng Li, Baorui Zou, Dashan Guo, Yingda Xu, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Yi Niu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2022 (Best Open Source Award) </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2207.06695.pdf" target="_blank" 
                               style="color: #990000">[Paper] </a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR" target="_blank" 
                               style="color: #990000">[Code] </a>
							<a href="" target="/files/mm2022_davarocr/DavarOCR-poster.pdf" 
                               style="color: #990000">[Poster] </a>
					
                        </small>
                    </div>
                </div>
                <hr />
				 <!--ACM MM 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2022_ctunet.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/acmmm2022_ctunet.html">End-to-End Compound Table Understanding with Multi-Modal Modeling</a>                
                            <br>
                            <small>Zaisheng Li<sup>*</sup>, Yi Li<sup>*</sup>, Liang Qiao<sup>*</sup>, Pengfei Li, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Xi Li<sup>†</sup></small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2022 (Oral) </font>
                            </strong>
			
                            <br>
                            <a class="unreleased">[Paper] (To be published)</a>
							<a class="unreleased" >[Code] (To be published) </a>
					
                        </small>
                    </div>
                </div>
                <hr /> 
        
				 <!--AAAI 2022 -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2022_open.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/aaai2022_pmal.html">PMAL: Open Set Recognition via Robust Prototype Mining</a>                
                            <br>
                            <small>Jing Lu<sup>*</sup>, Yunlu Xu<sup>*</sup>, Hao Li, Zhanzhan Cheng<sup>†</sup>, Yi Niu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2022  </font>
                            </strong>
			
                            <br>
                            <a href="https://arxiv.org/pdf/2203.08569.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a class="unreleased" >[Code] (To be published)</a>
							<a href="/files/aaai2022_pmal/pre-PMAL.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="/files/aaai2022_pmal/poster_PMAL.pdf" target="_blank" 
                               style="color: #990000">[Poster]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
        
				<!--BMVC 2021_FSL -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/bmvc2021.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/bmvc2021_FSL.html">A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning</a>                
                            <br>
                            <small>Linglan Zhao, Dashan Guo, Yunlu Xu<sup>†</sup>, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Xiangzhong Fang</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">BMVC 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2110.11128.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
				
			
				
				
			<!--ICDAR 2021_LGPMA -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_lgpma.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_lgpma.html">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment</a>                
                            <br>
                            <small>Liang Qiao<sup>*</sup>, Zaisheng Li<sup>*</sup>, Zhanzhan Cheng<sup>†</sup>, Peng Zhang, Shiliang Pu, Yi Niu, Wenqi Ren, Wenming Tan, and Fei Wu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral), Best Industry Paper</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06224.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/table_recognition/lgpma" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/icdar2021_lgpma/LGPMA-slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV19Q4y1Y73d/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
		
		<!--ICDAR 2021_VSR -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_vsr.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_vsr.html">VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations</a>                
                            <br>
                            <small>Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu and Fei Wu</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06220.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_layout/VSR" target="_blank" style="color: #990000" >[Code] </a>
							<a href="/files/icdar2021_vsr/VSR-Slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV1ff4y1n7zm/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 

		
		
		<!--ICDAR 2021_RF Learning -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icdar2021_rf.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="publication/icdar2021_rf.html">Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition</a>                
                            <br>
                            <small>Hui Jiang*, Yunlu Xu*, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu, Wenqi Ren, Fei Wu, Wenming Tan</small>
                        </h4>
                        <small>Accepted by
                            <strong>
                                <font color="red">ICDAR 2021 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2105.06229.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_recognition/rflearning" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/icdar2021_rfl/RFL-slide.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
							<a href="https://www.bilibili.com/video/BV1dq4y1M7kL/" target="_blank" 
                               style="color: #990000">[Video]</a>
                        </small>
                    </div>
                </div>
                <hr /> 
				
				
				

				<!-- AAAI 2021_MANGO -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2021_mango.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2021_mango.html">MANGO: A Mask Attention Guided One-Stage Scene Text Spotter

</a>                       
                            <br>
                            <small>Liang Qiao<sup>*</sup>, Ying Chen<sup>*</sup>, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu<sup>†</sup>, Fei Wu</small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2012.04350.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_spotting/mango" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/aaai2021_mango/MANGO-poster.pdf" target="_blank" 
                               style="color: #990000">[Poster]</a>
							<a href="/files/aaai2021_mango/MANGO-slides.pdf" target="_blank" 
                               style="color: #990000">[Slides]</a>
                       
                        </small>
                    </div>
                </div>
                <hr /> 
		    
		<!-- AAAI 2021_SPIN -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2021_spin.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2021_spin.html">SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition
</a>                       
                            <br>
                            <small>Chengwei Zhang*, Yunlu Xu*, Zhanzhan Cheng<sup>†</sup>, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2021</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2005.13117.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_recognition/spin" target="_blank" style="color: #990000">[Code] </a>
                       
                        </small>
                    </div>
                </div>
                <hr /> 
		<!-- ICPR 2020 FASDA-->
		<div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icpr2020_fasda.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/icpr2020_fasda.html">Text Recognition in Real Scenarios with a Few Labeled Samples</a>                       
                            <br>
                            <small>Jinghuang Lin, Zhanzhan Cheng, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ICPR 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2006.12209.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr /> 
		<!--ICPR 2020 PEE-->
		 <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/icpr2020_pee.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/icpr2020_pee.html">Recognizing Multiple Text Sequences from an Image by Pure End-To-End Learning</a>                       
                            <br>
                            <small>Zhenlong Xu, Shuigeng Zhou<sup>†</sup>, Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu</small>
                        </h4>

                        <small>Accepted by 
                            <strong>
                                <font color="red">ICPR 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1907.12791.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr /> 
		    
		<!-- AAM MM 2020_trie -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2020_trie.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2020_trie.html">TRIE: End-to-End Text Reading and Information Extraction for
Document Understanding</a>                       
                            <br>
                            <small>Peng Zhang<sup>*</sup>, Yunlu Xu<sup>*</sup>, Zhanzhan Cheng, Shiliang Pu<sup>†</sup>, Jing Lu, Liang Qiao, Yi Niu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2020</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/2005.13118.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/demo/text_ie/trie" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/mm20_trie/TRIE_slides.pdf" target="_blank"
                                style="color: #990000">[Slides]</a>
                        </small>
                    </div>
                </div>
                <hr />  
		    
                
                <!-- AAAI 2020_tp -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2020_tp.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2020_tp.html">Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting</a>                       
                            <br>
                            <small>Liang Qiao, Sanli Tang, Zhanzhan Cheng<sup>†</sup>, Yunlu Xu, Yi Niu, Shiliang Pu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2020 (Oral)</font>
                            </strong>
                            <br>
 		    <a href="https://arxiv.org/pdf/2002.06820" target="_blank" 
                               style="color: #990000">[Paper]</a>
                            <a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/text_detection/text_perceptron_det" target="_blank"
                                style="color: #990000">[Code]</a>
							<a href="/files/aaai2020_tp/AAAI20-TP-Supplementary.pdf" target="_blank"
                                style="color: #990000">[Supplementary]</a>
								<a href="/files/aaai2020_tp/AAAI20-TP-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
								<a href="https://www.bilibili.com/video/BV1Wz4y1k7Dc" target="_blank"
                                style="color: #990000">[Presentation Video]</a>                       
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
		<!--  PRCV 2019_REAPS -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/prcv2019_reaps.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/prcv2019_reaps.html">REAPS: Towards Better Recognition of Fine-grained Images by Region Attending and Part Sequencing</a>                       
                            <br>
                            <small>Peng Zhang, Xinyu Zhu, Zhanzhan Cheng<sup>†</sup>, Shuigeng Zhou, Yi Niu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">PRCV 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1908.01962.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a> 
							<a href="/files/pcrv19_reaps/prcv19-REAPS-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
                        </small>
                    </div>
                </div>
                <hr />  
		    
		
		<!--  ACMMM 2019_YORO -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2019_yoro.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2019_yoro.html">You Only Recognize Once: Towards Fast Video Text Spotting</a>                       
                            <br>
                            <small>Zhanzhan Cheng<sup>*</sup>, Jing Lu<sup>*</sup>, Yi Niu, Shiliang Pu, Fei Wu<sup>†</sup>, Shuigeng Zhou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1903.03299.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>  
							   <a href="dataset/lsvtd.html" target="_blank" 
                               style="color: #990000">[Dataset]</a>  
							<a href="https://github.com/hikopensource/DAVAR-Lab-OCR/tree/main/demo/videotext/yoro" target="_blank" style="color: #990000">[Code] </a>
							<a href="/files/mm19_yoro/MM19-YORO-poster.pdf" style="color: #990000" target="_blank">[Poster]</a>
							<a href="/files/mm19_yoro/MM19-YORO-spotlights.pdf" style="color: #990000" target="_blank">[Spotlights]</a>
							<a href="https://www.bilibili.com/video/bv1gK4y1L7No" target="_blank" style="color:#990000">[Demo]</a>
                        </small>
                    </div>
                </div>
                <hr />
                

		<!--  ACMMM 2019_SSG -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/acmmm2019_ssg.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/acmmm2019_ssg.html">Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal Action Localization</a>                       
                            <br>
                            <small>Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng<sup>†</sup>, Yi Niu, Shiliang Pu, Fei Wu, Futai Zou </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ACMMM 2019</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1908.02422.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							   <a href="/files/mm19_assg/MM19-ASSG-Poster.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>  
								<a href="/files/mm19_assg/MM19-ASSG-spotlights.pdf" target="_blank"
                                style="color: #990000">[Spotlights]</a>								
                        </small>
                    </div>
                </div>
                <hr />
		    
		    
		                <!-- AAAI 2019_star -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/aaai2019_star.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/aaai2019_star.html">Segregated Temporal Assembly Recurrent Networks for Weakly Supervised Multiple Action Detection</a>                       
                            <br>
                            <small>Yunlu Xu<sup>*</sup>, Chengwei Zhang<sup>*</sup>, Zhanzhan Cheng<sup>†</sup>, Jianwen Xie, Yi Niu, Shiliang Pu, Fei Wu </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">AAAI 2019 (Oral)</font>
                            </strong>
                            <br>
                            <a href="https://arxiv.org/pdf/1811.07460.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							 <a href="/files/aaai2019_star/AAAI19-Star-Slides.pdf" target="_blank"
                                style="color: #990000">[Slides]</a>  
                        </small>
                    </div>
                </div>
                <hr />  
		    
		 
				                <!-- CVPR 2018_EPL -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2018_epl.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/cvpr2018_epl.html">Edit Probability for Scene Text Recognition</a>                       
                            <br>
                            <small>Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2018</font>
                            </strong>
                            <br>
                            <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Edit_Probability_for_CVPR_2018_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
						                <!-- CVPR 2018_AON -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/cvpr2018_aon.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/cvpr2018_aon.html">AON: Towards Arbitrarily-Oriented Text Recognition</a>                       
                            <br>
                            <small>Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">CVPR 2018</font>
                            </strong>
                            <br>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="/files/cvpr18_aon/cvpr18_poster_AON.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
                        
                        </small>
                    </div>
                </div>
                <hr />  
		    
		    
								                <!-- ICCV 2017_FAN -->
                <div class="media">
                    <div class="media-body">
                        <div class="col-xs-3">
                            <img src="/img/publications/iccv2017_fan.png">
                        </div>

                        <h4 class="media-heading">
			    <a href="/publication/iccv2017_fan.html">Focusing Attention: Towards Accurate Text Recognition in Natural Images</a>                       
                            <br>
                            <small>Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, Shuigeng Zhou<sup>†</sup> </small>
                        </h4>

                        <small>Accepted by
                            <strong>
                                <font color="red">ICCV 2017</font>
                            </strong>
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf" target="_blank" 
                               style="color: #990000">[Paper]</a>
							<a href="/files/iccv17_fan/iccv17_poster_FAN.pdf" target="_blank"
                                style="color: #990000">[Poster]</a>
							
                        </small>
                    </div>
                </div>
                <hr />  
            </div>
 
            <div class="item">
                规格与包装 主体 首销日期：23日 产品名称：iPhone 12 上市月份：10月 入网型号：A2404 上市年份：2020年
            </div>
            <div class="item">
                商品评价 运行非常流畅，轻松到手⑤千1，是从底下视频拿到的！拍照效果一流，这款刚出来的紫色非常喜欢，外形设计漂亮，屏幕显示出众，音质也非常完美，不错的。手机包装也很好，发货速度很快，非常的漂亮，很喜欢。双摄像头拍照，很快5G运行，速度也很棒非常的满意。双扬声器音质很棒，新的外观设计看起来非常的绚丽，手感也很好，玻璃质感很好。系统确认比安卓的流畅多了，尺寸刚刚好，不会太大屏幕细腻，色彩还原度高，采用高通基带，5G信号还是不错的，上网速度快特别喜欢紫色，颜值特别高没办法不热爱。当收藏款来购买的啊，这次京东商城活动力度大就入手了。非常流畅，屏幕显示细腻，看着很舒服，夜景提升很大，拍出来的效果非常符合苹果的水准，和以往的苹果无home键，需要点时间去适应。手机外型尺寸正合适，对经典的直角边设计也非常适应。使用感受特别好，开机顺畅，按键灵敏，运行速度快，待机时间长，音效好，拍照清晰，内存足够充足！立刻下载了好多实用软件，非常开心！太小刚刚好，挺顺手的，经典的直边边框手感不错，苹果的优势是系统，系统不用说了，运行流畅的代名词。非常流畅，屏幕显示细腻，看着很舒服，夜景提升很大，拍出来的效果非常符合苹果的水准。
            </div>
            <div class="item">
                售后保障模块内容 厂家服务 本商品质保周期为1年质保，在此时间范围内可提交维修申请，具体请以厂家服务为准。 如因质量问题或故障，凭厂商维修中心或特约维修点的质量检测证明，享受7日内退货，15日内换货，15日以上在质保期内享受免费保修等三包服务！ (注:如厂家在商品介绍中有售后保障的说明,则此商品按照厂家说明执行售后保障服务。)
            </div>
        </div>
    </div>

    </div>


    <div class="footer"></div>
    

    <script>
		
		
		var tab_list = document.querySelector('.tab_list');
        var list = tab_list.querySelectorAll('li');//获取所有的选项卡标签
 
        var items = document.querySelectorAll('.item');//获取所有的内容显示模块
		//在循环中添加点击事件，为每个选项卡添加索引index
        for (var i = 0; i < list.length; i++) {
            list[i].setAttribute('index', i);
            //排他思想
            list[i].onclick = function() {
            	//1.点击后先消除所有选项卡的样式
                for (var i = 0; i < list.length; i++) {
                    list[i].className = '';
                }
                //2.绑定当前选项卡的样式(实现背景色变红)
                this.className = 'current';
                
                //内容显示模块
                var index = this.getAttribute('index');//先获得当前点击选项卡的索引值
                //1.将所有模块隐藏
                for (var i = 0; i < items.length; i++) {
                    items[i].style.display = 'none';
                }
                //2.再显示对应index下的模块
                items[index].style.display = 'block';
            }
        }
    </script>
</body>

</html>
